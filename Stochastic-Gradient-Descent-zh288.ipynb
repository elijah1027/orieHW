{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.extmath import safe_sparse_dot, squared_norm\n",
    "from scipy.misc import comb, logsumexp \n",
    "from sklearn.linear_model.logistic import _multinomial_grad_hess\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "X = mnist.data.astype('float64')\n",
    "y = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_samples = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_state = check_random_state(0)\n",
    "\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "X = X.reshape((X.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=train_samples, test_size=10000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = np.zeros((len(y_train), 10))\n",
    "for i,j in enumerate(y_train):\n",
    "    Y_train[i, int(j)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test = np.zeros((len(y_test), 10))\n",
    "for i,j in enumerate(y_test):\n",
    "    Y_test[i, int(j)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(X,Y, w, alpha=0):\n",
    "    n_classes = Y.shape[1]\n",
    "    n_features = X.shape[1]\n",
    "    m= X.shape[0]\n",
    "    w = w.reshape(n_classes, -1)\n",
    "    fit_intercept = w.size == (n_classes * (n_features + 1))\n",
    "    old_w = w.copy()\n",
    "    if fit_intercept:\n",
    "        intercept = w[:, -1]\n",
    "        w = w[:, :-1]\n",
    "    else:\n",
    "        intercept = 0\n",
    "    p = safe_sparse_dot(X, w.T)\n",
    "    p += intercept\n",
    "    p -= logsumexp(p, axis=1)[:, np.newaxis]\n",
    "    loss = (-1/m)*(Y * p).sum()\n",
    "    loss += 0.5 * alpha * squared_norm(w)\n",
    "    p = np.exp(p, p)\n",
    "    return loss, p, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_loss(X, Y, w, alpha=0):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #using sigmoid function from the first hw\n",
    "    scores = np.dot(X,w)\n",
    "    scores -= np.max(scores)\n",
    "    prob = (np.exp(scores).T / np.sum(np.exp(scores),axis=1)).T\n",
    "    \n",
    "    grad = -np.dot(X.T,(Y - prob)) + alpha*w\n",
    "#     print(Y.shape)\n",
    "#     print(prob.shape)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hessian_loss(X, Y, w, alpha=0):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #http://www.cedar.buffalo.edu/~srihari/CSE574/Chap4/4.3.4-MultiLogistic.pdf\n",
    "    #page 19 hessian function of multi logistic regression\n",
    "    scores = np.dot(X,w)\n",
    "    scores -= np.max(scores)\n",
    "    prob = (np.exp(scores).T / np.sum(np.exp(scores),axis=1)).T\n",
    "    horizontal = []\n",
    "    vertical = []\n",
    "    for i in range(0,10):\n",
    "        for j in range(0,10):\n",
    "            if i == j:\n",
    "                ind = 1\n",
    "            else:\n",
    "                ind = 0\n",
    "            \n",
    "            part = -safe_sparse_dot((X*prob[:,i:(i+1)]).T,X)*ind+safe_sparse_dot((X*prob[:,i:(i+1)]).T,X*prob[:,j:(j+1)])+alpha\n",
    "            horizontal.append(part)\n",
    "        \n",
    "        vertical.append(np.concatenate(horizontal,axis=1))\n",
    "        horizontal = []\n",
    "    return np.concatenate(vertical,axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_intercept = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(X,Y,w,momentum=0.9, lr=0.01, batch_size=1001, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Real-time forward-mode algorithm using stochastic gradient descent with constant learning \n",
    "    rate. Observe that you should only find the optimal learning rate (lr), and \n",
    "    penalty parameter (alpha). \n",
    "    \n",
    "    We use the SGD with momentum, which is defined here: \n",
    "    http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/\n",
    "    \"\"\"\n",
    "    #first use gd to train hyper parameters\n",
    "    iterations = 100\n",
    "    for i in range(0,iterations):\n",
    "        hyper = hyper_train(X,Y,momentum,lr,batch_size,alpha)\n",
    "        lr = lr - hyper[0]\n",
    "        alpha = alpha - hyper[1]\n",
    "    #use optimized hyper parameters to do sgd\n",
    "    #initialize a new w\n",
    "    w = np.random.rand(X_train.shape[1],Y_train.shape[1])\n",
    "    for i in range(0,iterations):\n",
    "        #random sample batch\n",
    "        print(\"sgs iterations: {}\".format(i))\n",
    "        #random sample. Code copied from https://stackoverflow.com/questions/14262654/numpy-get-random-set-of-rows-from-2d-array\n",
    "        idx = np.random.randint(X_train.shape[0], size=batch_size)\n",
    "        X = X_train[idx,:]\n",
    "        Y = Y_train[idx,:]\n",
    "        v = momentum*v + lr*grad_loss(X,Y,w,alpha)\n",
    "        w = w - v\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyper_train(X,Y,momentum=0.9, lr=0.01, batch_size=1001, alpha=0.1):\n",
    "    #initilize w and v\n",
    "    w = np.zeros((X_train.shape[1],Y_train.shape[1]))\n",
    "    v = np.zeros((X_train.shape[1],Y_train.shape[1]))\n",
    "    #random sample. Code copied from https://stackoverflow.com/questions/14262654/numpy-get-random-set-of-rows-from-2d-array\n",
    "    idx = np.random.randint(X_train.shape[0], size=batch_size)\n",
    "    X = X_train[idx,:]\n",
    "    Y = Y_train[idx,:]\n",
    "    z = np.zeros((X_train.shape[1]*10*2,2))\n",
    "    iterations = 100\n",
    "    for i in range(0,iterations):\n",
    "        \n",
    "        print(\"hyper_train iterations: {}\".format(i))\n",
    "        \n",
    "        #-calculate A\n",
    "        hessian = hessian_loss(X,Y,w,alpha)\n",
    "        t1 = np.concatenate([momentum*np.identity(X_train.shape[1]*10),-momentum*np.identity(X_train.shape[1]*10)])\n",
    "        t2 = np.concatenate([hessian,np.identity(X_train.shape[1]*10)-hessian])\n",
    "        A = np.concatenate([t1,t2],axis=1)\n",
    "        #-calculate B\n",
    "        gradient_nonF = grad_loss(X,Y,w,alpha)\n",
    "        gradient = gradient_nonF.flatten()\n",
    "        print(gradient)\n",
    "        w_F = w.flatten()\n",
    "        t1 = np.concatenate([gradient,-gradient])\n",
    "        t2 = np.concatenate([w_F,-w_F])\n",
    "        B = np.array([t1,t2]).T\n",
    "        #-calculate Z\n",
    "        z = np.dot(A,z)+B\n",
    "        #-Although paper says updating S_t first, A_t and B_t is derivative w.r.t parameters in t-1. Therefore, I calculate A_t,B_t before updating the w andv.\n",
    "        v = momentum*v + lr*gradient_nonF\n",
    "        w = w - v\n",
    "        #-Validation error does not contain v. Gradient of validation error w.r.t S_t is zeroes followed by gradients w.r.t to w.\n",
    "    grad_val = np.concatenate([np.array([0]*X_train.shape[1]*10),gradient]).reshape(1,X_train.shape[1]*10*2)\n",
    "    return safe_sparse_dot(grad_val,z)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
